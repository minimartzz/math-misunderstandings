{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Solutions\n",
    "\n",
    "Ideas taken from:\n",
    "- https://www.kaggle.com/code/minglv/ultimate-ensemble-fusion/notebook\n",
    "- https://www.kaggle.com/code/verracodeguacas/guarded-assembly-map-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | User | Change Type | Remarks |  \n",
    "| ---- | ---- | ----------- | ------- |\n",
    "| 30/09/2025   | Martin | Created   | Notebook created to try solutions from Kaggle | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Content\n",
    "\n",
    "* [Introduction](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Software\\venv\\py311_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/raw\"\n",
    "train = pl.read_csv(f\"{path}/train.csv\")\n",
    "test = pl.read_csv(f\"{path}/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15156\\2339516059.py:15: DeprecationWarning: `GroupBy.count` is deprecated. It has been renamed to `len`.\n",
      "  temp = temp.group_by(['QuestionId', 'MC_Answer']).count().sort('count', descending=True)\n"
     ]
    }
   ],
   "source": [
    "# Add new labels\n",
    "le = LabelEncoder()\n",
    "train = train.with_columns(\n",
    "  Target = pl.col('Category') + \":\" + pl.col('Misconception'),\n",
    "  Correct = pl.col(\"Category\").str.split(\"_\").list.last() == \"Correct\"\n",
    ")\n",
    "train = train.with_columns(\n",
    "  Label = pl.col(\"Target\").map_batches(le.fit_transform)\n",
    ")\n",
    "\n",
    "# Get known answers\n",
    "temp = train.filter(\n",
    "  pl.col('Correct')\n",
    ")\n",
    "temp = temp.group_by(['QuestionId', 'MC_Answer']).count().sort('count', descending=True)\n",
    "temp = temp.unique(subset=['QuestionId'])\n",
    "temp = temp.drop('count')\n",
    "temp = temp.with_columns(\n",
    "  Correct=1\n",
    ")\n",
    "\n",
    "# Evaluate correctness of test set answer\n",
    "test = test.join(\n",
    "  temp,\n",
    "  how='left',\n",
    "  on=['QuestionId', 'MC_Answer']\n",
    ")\n",
    "test = test.fill_null(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(df):\n",
    "  df = df.with_columns(\n",
    "    pl.when(pl.col(\"Correct\") == 1)\n",
    "      .then(pl.lit(\"This answer is correct.\"))\n",
    "      .otherwise(pl.lit(\"This answer is wrong.\"))\n",
    "      .alias(\"Correctness\")\n",
    "  )\n",
    "\n",
    "  return df.with_columns(\n",
    "    pl.format(\n",
    "      \"Question:\\n{}\\nAnswer:\\n{}\\nCorrect:\\n{}\\nExplanation:\\n{}\",\n",
    "      df['QuestionText'],\n",
    "      df['MC_Answer'],\n",
    "      df['Correctness'],\n",
    "      df['StudentExplanation']\n",
    "    ).alias(\"text\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = format_input(train)\n",
    "test = format_input(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model_path, test, model_type=\"standard\"):\n",
    "  print(f\"Loading model from {model_path}\")\n",
    "\n",
    "  # Clear memory\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "\n",
    "  # Load tokenizer\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "  if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "  \n",
    "  # Try PEFT, else fallback standard\n",
    "  if model_type == 'peft':\n",
    "    try:\n",
    "      print(\"Attempting PEFT model loading\")\n",
    "      base_model_paths = [\n",
    "        model_path,\n",
    "        \"/kaggle/input/gemma2-9b-it-bf16\", \n",
    "        \"google/gemma-2-9b-it\"\n",
    "      ]\n",
    "      model = None\n",
    "      for base_path in base_model_paths:\n",
    "        try:\n",
    "          print(f\"Trying base model: {base_path}\")\n",
    "          base_model = AutoModelForSequenceClassification(\n",
    "            base_path,\n",
    "            num_labels=65,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "          )\n",
    "          model = PeftModel.from_pretrained(base_model, model_path)\n",
    "          print(f\"PEFT loaded successfully with base: {base_path}\")\n",
    "          break\n",
    "        except Exception as e:\n",
    "          print(f\"Failed with base {base_path}: {str(e)[:100]}\")\n",
    "          continue\n",
    "      if model is None:\n",
    "        raise Exception(\"All PEFT attempts failed\")\n",
    "    except Exception as e:\n",
    "      print(f\"Falling back to standard model\")\n",
    "      model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "      )\n",
    "  else:\n",
    "    print(f\"Using standard model\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "      model_path,\n",
    "      device_map=\"auto\",\n",
    "      torch_dtype=torch.bfloat16,\n",
    "      num_labels=65\n",
    "    )\n",
    "  \n",
    "  model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "  def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "  \n",
    "  test_tokenized = test.map(tokenize, batched=True)\n",
    "\n",
    "  # Trainer and inference\n",
    "  trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "      output_dir='results',\n",
    "      do_predict=True,\n",
    "      per_device_eval_batch_size=2,\n",
    "      fp16=True,\n",
    "      report_to=\"none\"\n",
    "    ),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer)\n",
    "  )\n",
    "\n",
    "  print(\"Inference\")\n",
    "  predictions = trainer.predict(test_tokenized)\n",
    "  logits = predictions.predictions\n",
    "\n",
    "  del model, trainer, tokenizer, test_tokenized\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  # print(f\"Inference completed. Shape: {logits.shape}\")\n",
    "  return logitsdef get_predictions(model_path, test, model_type=\"standard\"):\n",
    "  print(f\"Loading model from {model_path}\")\n",
    "\n",
    "  # Clear memory\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "\n",
    "  # Load tokenizer\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "  if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "  \n",
    "  # Try PEFT, else fallback standard\n",
    "  if model_type == 'peft':\n",
    "    try:\n",
    "      print(\"Attempting PEFT model loading\")\n",
    "      base_model_paths = [\n",
    "        model_path,\n",
    "        \"/kaggle/input/gemma2-9b-it-bf16\", \n",
    "        \"google/gemma-2-9b-it\"\n",
    "      ]\n",
    "      model = None\n",
    "      for base_path in base_model_paths:\n",
    "        try:\n",
    "          print(f\"Trying base model: {base_path}\")\n",
    "          base_model = AutoModelForSequenceClassification(\n",
    "            base_path,\n",
    "            num_labels=65,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "          )\n",
    "          model = PeftModel.from_pretrained(base_model, model_path)\n",
    "          print(f\"PEFT loaded successfully with base: {base_path}\")\n",
    "          break\n",
    "        except Exception as e:\n",
    "          print(f\"Failed with base {base_path}: {str(e)[:100]}\")\n",
    "          continue\n",
    "      if model is None:\n",
    "        raise Exception(\"All PEFT attempts failed\")\n",
    "    except Exception as e:\n",
    "      print(f\"Falling back to standard model\")\n",
    "      model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "      )\n",
    "  else:\n",
    "    print(f\"Using standard model\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "      model_path,\n",
    "      device_map=\"auto\",\n",
    "      torch_dtype=torch.bfloat16,\n",
    "      num_labels=65\n",
    "    )\n",
    "  \n",
    "  model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "  def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "  \n",
    "  test_tokenized = test.map(tokenize, batched=True)\n",
    "\n",
    "  # Trainer and inference\n",
    "  trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "      output_dir='results',\n",
    "      do_predict=True,\n",
    "      per_device_eval_batch_size=2,\n",
    "      fp16=True,\n",
    "      report_to=\"none\"\n",
    "    ),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer)\n",
    "  )\n",
    "\n",
    "  print(\"Inference\")\n",
    "  predictions = trainer.predict(test_tokenized)\n",
    "  logits = predictions.predictions\n",
    "\n",
    "  del model, trainer, tokenizer, test_tokenized\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "  \n",
    "  # print(f\"Inference completed. Shape: {logits.shape}\")\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = Dataset.from_polars(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Ensemble\n",
    "# paths\n",
    "model_1 = \"deepseek-ai/deepseek-math-7b-base\"\n",
    "model_2 = \"justsomerandomdude264/Math_Homework_Solver_Llama318B\"\n",
    "model_3 = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "\n",
    "# =============== Model 1 ===============\n",
    "predictions_1 = get_predictions(model_1, ds_test, \"standard\")\n",
    "\n",
    "# =============== Model 2 ===============\n",
    "predictions_2 = get_predictions(model_2, ds_test, \"standard\")\n",
    "\n",
    "# =============== Model 3 ===============\n",
    "predictions_3 = get_predictions(model_3, ds_test, \"standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting results\n",
    "model_1_weight = 0.1\n",
    "model_2_weight = 0.6\n",
    "model_3_weight = 0.3\n",
    "\n",
    "ensemble_prediction = (model_1_weight * predictions_1 + model_2_weight * predictions_2 + model_3_weight * predictions_3)\n",
    "\n",
    "top_indices = np.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
